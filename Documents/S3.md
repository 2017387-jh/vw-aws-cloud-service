# S3 (Simple Storage Service) 스크립트 가이드

## 개요
Amazon S3는 이 프로젝트의 핵심 스토리지 서비스로, 입력 이미지와 처리된 출력 이미지를 저장하는 역할을 담당합니다. 두 개의 분리된 버킷을 사용하여 입력과 출력을 명확히 구분하고, 보안 설정을 통해 안전한 데이터 관리를 제공합니다.

## 📁 관련 파일
```
script/
├── s3_create_bucket.sh     # S3 버킷 생성
├── s3_delete_bucket.sh     # S3 버킷 삭제
├── s3_upload_file.sh       # 파일 업로드 (직접)
└── s3_download_file.sh     # 파일 다운로드 (직접)
```

## 🏗️ s3_create_bucket.sh

### 기능
- 입력/출력용 S3 버킷 생성
- 보안 설정 (퍼블릭 접근 차단)
- 버전 관리 활성화

### 상세 분석

#### 1. 환경 설정
```bash
#!/usr/bin/env bash
set -euo pipefail
source .env
aws configure set region "$AWS_REGION"
```
- 엄격한 에러 처리 모드
- 환경 변수 로드
- AWS CLI 리전 설정

#### 2. 버킷 생성 루프
```bash
for b in "$DDN_IN_BUCKET" "$DDN_OUT_BUCKET"; do
  echo "[INFO] Creating bucket: $b"
  aws s3api create-bucket \
    --bucket "$b" \
    --create-bucket-configuration LocationConstraint="$AWS_REGION" || true
```
- **반복 처리**: 입력 및 출력 버킷을 동일한 로직으로 생성
- **리전 제약**: 버킷을 특정 리전에 생성
- **멱등성**: `|| true`로 이미 존재하는 버킷도 오류 없이 처리

**버킷명**:
- `ddn-in-bucket`: 클라이언트가 업로드하는 원본 이미지 저장
- `ddn-out-bucket`: 처리된 결과 이미지 저장

#### 3. 퍼블릭 접근 차단 설정
```bash
aws s3api put-public-access-block --bucket "$b" \
  --public-access-block-configuration BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true
```

**보안 설정 상세**:
- **BlockPublicAcls=true**: 새로운 퍼블릭 ACL 업로드 및 적용 차단
- **IgnorePublicAcls=true**: 기존 퍼블릭 ACL 무시
- **BlockPublicPolicy=true**: 퍼블릭 접근을 허용하는 버킷 정책 차단  
- **RestrictPublicBuckets=true**: 계정 레벨에서 퍼블릭 정책 적용 금지

이 설정을 통해 데이터 유출 방지 및 AWS 보안 모범 사례 준수

#### 4. 버전 관리 활성화
```bash
aws s3api put-bucket-versioning --bucket "$b" \
  --versioning-configuration Status=Enabled
```
- **목적**: 파일 덮어쓰기 시 이전 버전 보존
- **장점**: 실수로 삭제/수정된 파일 복구 가능
- **비용**: 여러 버전 저장으로 스토리지 비용 증가

### 보안 아키텍처
```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Client App    │    │   Lambda/API     │    │  ECS Container  │
│                 │    │   (Presigned     │    │  (Processing)   │
│                 │    │    URL Only)     │    │                 │
└─────────────────┘    └──────────────────┘    └─────────────────┘
         │                       │                       │
         │ Presigned URL         │ Direct Access         │ Direct Access
         │ (Temporary)           │ (IAM Role)           │ (IAM Role)
         ▼                       ▼                       ▼
┌─────────────────────────────────────────────────────────────────┐
│                    S3 Buckets                                   │
│  ┌─────────────────┐              ┌─────────────────┐          │
│  │ ddn-in-bucket   │─────────────▶│ ddn-out-bucket  │          │
│  │ (Input Images)  │   Processing │ (Output Images) │          │
│  │                 │              │                 │          │
│  │ - Public Block  │              │ - Public Block  │          │
│  │ - Versioning    │              │ - Versioning    │          │
│  └─────────────────┘              └─────────────────┘          │
└─────────────────────────────────────────────────────────────────┘
```

---

## 🗑️ s3_delete_bucket.sh

### 기능
- S3 버킷 및 내부 객체 완전 삭제
- 안전한 정리 작업

### 상세 분석

#### 1. 버킷별 삭제 루프
```bash
for b in "$DDN_IN_BUCKET" "$DDN_OUT_BUCKET"; do
  echo "[INFO] Deleting bucket: $b"
  aws s3 rm "s3://$b" --recursive || true
  aws s3 rb "s3://$b" --force || true
done
```

**삭제 순서**:
1. **객체 삭제**: `aws s3 rm --recursive`
   - 버킷 내 모든 객체 및 버전 삭제
   - 재귀적으로 모든 폴더/파일 처리
2. **버킷 삭제**: `aws s3 rb --force`
   - 빈 버킷 삭제
   - `--force`: 버전 관리된 객체도 강제 삭제

#### 2. 에러 처리
```bash
|| true
```
- 버킷이 존재하지 않아도 스크립트 계속 실행
- 정리 스크립트의 안전성 보장

### 주의사항
⚠️ **데이터 손실 위험**: 이 스크립트는 모든 데이터를 영구적으로 삭제합니다.
- 운영 환경에서는 백업 확인 후 실행
- 중요한 데이터는 별도 백업 권장

---

## 📤 s3_upload_file.sh

### 기능
- 로컬 파일을 S3 입력 버킷에 직접 업로드
- 개발/테스트 용도의 간단한 업로드 도구

### 상세 분석

#### 1. 파라미터 처리
```bash
FILE_PATH=${1:-test_input.tif}
S3_KEY=${2:-user/test_input.tif}
```
- **첫 번째 인자**: 업로드할 로컬 파일 경로
- **두 번째 인자**: S3에서 사용할 키(경로/파일명)
- **기본값**: 인자가 없을 경우 테스트 파일 사용

#### 2. 파일 업로드
```bash
echo "[INFO] Uploading $FILE_PATH to s3://$DDN_IN_BUCKET/$S3_KEY"
aws s3 cp "$FILE_PATH" "s3://$DDN_IN_BUCKET/$S3_KEY"
```
- AWS CLI의 `s3 cp` 명령어 사용
- 자동 멀티파트 업로드 (대용량 파일)
- 진행률 표시 기능

### 사용 예시
```bash
# 기본 파일 업로드
./s3_upload_file.sh

# 특정 파일 업로드
./s3_upload_file.sh /path/to/image.tif custom/path/image.tif

# 데모 이미지 업로드
./s3_upload_file.sh demo_image/static_demo_140um_madible_VD.tif demo/test.tif
```

---

## 📥 s3_download_file.sh

### 기능
- S3 입력 버킷에서 로컬로 파일 다운로드
- 테스트 및 검증 용도

### 상세 분석

#### 1. 파라미터 처리
```bash
S3_KEY=${1:-raw/sample.txt}
LOCAL_PATH=${2:-download.txt}
```
- **첫 번째 인자**: S3에서 다운로드할 객체 키
- **두 번째 인자**: 저장할 로컬 경로
- **기본값**: 샘플 파일 다운로드

#### 2. 파일 다운로드
```bash
echo "[INFO] Downloading s3://$DDN_IN_BUCKET/$S3_KEY to $LOCAL_PATH"
aws s3 cp "s3://$DDN_IN_BUCKET/$S3_KEY" "$LOCAL_PATH"
```
- 지정된 S3 객체를 로컬에 복사
- 네트워크 중단 시 자동 재시도
- 체크섬 검증으로 무결성 보장

### 사용 예시
```bash
# 기본 파일 다운로드
./s3_download_file.sh

# 특정 파일 다운로드
./s3_download_file.sh user/processed_image.tif ./result.tif

# 처리된 이미지 다운로드
./s3_download_file.sh demo/processed.tif ./processed_demo.tif
```

---

## 🔄 워크플로우 시나리오

### 1. 초기 설정
```bash
# S3 버킷 생성
./s3_create_bucket.sh

# 버킷 확인
aws s3 ls | grep ddn-
```

### 2. 개발/테스트 워크플로우
```bash
# 1. 테스트 이미지 업로드
./s3_upload_file.sh demo_image/static_demo_140um_madible_VD.tif test/input.tif

# 2. 처리 진행 (Lambda/ECS를 통한 처리)
# ... 이미지 처리 과정 ...

# 3. 결과 이미지 다운로드
./s3_download_file.sh test/output.tif ./result.tif
```

### 3. 프로덕션 워크플로우
```
Client → Presigned URL (Lambda) → Direct Upload (S3) 
   ↓
S3 Input Bucket → ECS Processing → S3 Output Bucket
   ↓
Presigned URL (Lambda) ← Client Download ← S3 Output
```

### 4. 정리 작업
```bash
# 개발이 완료되면 버킷 삭제
./s3_delete_bucket.sh
```

---

## 🔧 고급 설정

### 1. 생명 주기 정책 설정
```bash
# 30일 후 Standard-IA로 이동, 90일 후 Glacier로 아카이브
aws s3api put-bucket-lifecycle-configuration \
  --bucket ddn-in-bucket \
  --lifecycle-configuration file://lifecycle.json
```

**lifecycle.json 예시**:
```json
{
  "Rules": [
    {
      "ID": "ImageProcessingLifecycle",
      "Status": "Enabled",
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        }
      ]
    }
  ]
}
```

### 2. 암호화 설정
```bash
# AES-256 서버 측 암호화
aws s3api put-bucket-encryption \
  --bucket ddn-in-bucket \
  --server-side-encryption-configuration '{
    "Rules": [
      {
        "ApplyServerSideEncryptionByDefault": {
          "SSEAlgorithm": "AES256"
        }
      }
    ]
  }'
```

### 3. 교차 리전 복제
```bash
# 재해 복구를 위한 다른 리전으로 복제 설정
aws s3api put-bucket-replication \
  --bucket ddn-in-bucket \
  --replication-configuration file://replication.json
```

### 4. CloudTrail 연동
```bash
# S3 API 호출 로깅
aws cloudtrail create-trail \
  --name ddn-s3-audit \
  --s3-bucket-name ddn-audit-logs
```

---

## 🛡️ 보안 모범 사례

### 1. IAM 정책 최소화
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": [
        "arn:aws:s3:::ddn-in-bucket/*",
        "arn:aws:s3:::ddn-out-bucket/*"
      ]
    }
  ]
}
```

### 2. VPC 엔드포인트 사용
```bash
# S3용 VPC 엔드포인트 생성
aws ec2 create-vpc-endpoint \
  --vpc-id vpc-026400b6f5ea5c7f6 \
  --service-name com.amazonaws.ap-northeast-2.s3 \
  --route-table-ids rtb-12345678
```

### 3. 액세스 로깅
```bash
# S3 액세스 로그 활성화
aws s3api put-bucket-logging \
  --bucket ddn-in-bucket \
  --bucket-logging-status '{
    "LoggingEnabled": {
      "TargetBucket": "ddn-access-logs",
      "TargetPrefix": "access-logs/"
    }
  }'
```

---

## ⚠️ 주의사항

### 1. 비용 관리
- **스토리지 비용**: 사용하지 않는 파일 정기적 정리
- **요청 비용**: GET/PUT 요청 횟수 모니터링
- **데이터 전송**: 리전 간 전송 시 추가 비용

### 2. 성능 최적화
- **멀티파트 업로드**: 대용량 파일 (100MB+) 시 자동 적용
- **Transfer Acceleration**: 글로벌 업로드 성능 향상
- **CloudFront**: 다운로드 성능 및 비용 최적화

### 3. 데이터 관리
- **버전 관리**: 스토리지 비용 증가 요인
- **중복 제거**: 동일한 파일 업로드 방지
- **백업 전략**: 중요 데이터 다중 백업

---

## 🔍 트러블슈팅

### 1. 버킷 생성 실패
```bash
# 버킷명 중복 확인
aws s3api head-bucket --bucket ddn-in-bucket

# 리전별 제약 확인
aws s3api get-bucket-location --bucket ddn-in-bucket
```

### 2. 업로드/다운로드 실패
```bash
# 권한 확인
aws s3api get-bucket-policy --bucket ddn-in-bucket

# 네트워크 연결 테스트
aws s3 ls s3://ddn-in-bucket/
```

### 3. 퍼블릭 접근 문제
```bash
# 퍼블릭 접근 차단 상태 확인
aws s3api get-public-access-block --bucket ddn-in-bucket

# 버킷 정책 확인
aws s3api get-bucket-policy --bucket ddn-in-bucket
```

---

## 📊 모니터링

### 1. CloudWatch 메트릭
- **BucketSizeBytes**: 버킷 크기 모니터링
- **NumberOfObjects**: 객체 수 추적
- **AllRequests**: API 요청 수 모니터링

### 2. 비용 분석
```bash
# AWS Cost Explorer API를 통한 S3 비용 분석
aws ce get-cost-and-usage \
  --time-period Start=2024-01-01,End=2024-01-31 \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=SERVICE
```

### 3. 사용량 보고서
```bash
# S3 인벤토리 설정
aws s3api put-bucket-inventory-configuration \
  --bucket ddn-in-bucket \
  --id DailyInventory \
  --inventory-configuration file://inventory.json
```

---

## 🚀 자동화 스크립트

### 1. 배치 업로드
```bash
#!/bin/bash
# 폴더 내 모든 이미지 업로드
for file in images/*.tif; do
  filename=$(basename "$file")
  ./s3_upload_file.sh "$file" "batch/$filename"
done
```

### 2. 정리 작업
```bash
#!/bin/bash
# 7일 이상된 임시 파일 삭제
aws s3api list-objects-v2 --bucket ddn-in-bucket --prefix temp/ \
  --query "Contents[?LastModified<='$(date -d '7 days ago' -Iseconds)'].Key" \
  --output text | xargs -I {} aws s3 rm s3://ddn-in-bucket/{}
```

### 3. 백업 스크립트
```bash
#!/bin/bash
# 다른 리전으로 중요 파일 백업
aws s3 sync s3://ddn-out-bucket/important/ \
  s3://ddn-backup-bucket-us-west-2/backup/ \
  --region us-west-2
```